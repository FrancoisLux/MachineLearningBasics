{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Given input x, find the probability that y = 1, where 0 and 1 denote the two possibilities of a binary classification problem.\n",
    "\n",
    "In order to obtain a probability between 0 and 1 we use the sigmoid function $$\\hat{y} = a = \\sigma(z) = \\frac{1}{1+e^{-z}}$$ with z being linear $$z = wx +b$$\n",
    "\n",
    "### Loss-function \n",
    "\n",
    "The loss-function for one sample can be given by $$L(\\hat{y},y)=\\frac{1}{2}(\\hat{y} - y)^2$$\n",
    "\n",
    "But then the optimization problem becomes non-convex (many local optima). Instead we use \n",
    "\n",
    "\n",
    "$$L(\\hat{y},y)=-(ylog\\hat{y} + (1-y)log(1-\\hat{y}))$$\n",
    "\n",
    "\n",
    "If y = 1, then this simplifies to $L(\\hat{y},y)=-log\\hat{y}$, so a large $\\hat{y}$ is wanted to minimize loss-function.\n",
    "\n",
    "If y = 0, the equation simplifies to $L(\\hat{y},y)=-log(1-\\hat{y}))$, so a small $\\hat{y}$ minimizes the loss-function.\n",
    "\n",
    "### Cost-function \n",
    "\n",
    "The cost-function for all test-data: \n",
    "\n",
    "$$J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^m L(\\hat{y},y)$$\n",
    "\n",
    "Initializing the weights and the bias with 0 or at random (does not matter too much because the Loss-function is convex) and then calculating the Cost-function given m samples having $n_x$-features, constitutes the Forward-step of Logistic-Regression.\n",
    "\n",
    "The Backwards-step updates the weights and bias.\n",
    "\n",
    "  $$w_i = w -\\alpha \\frac{dJ(w, b)}{dw_i}$$  \n",
    "  $$b = b -\\alpha \\frac{dJ(w, b)}{db}$$\n",
    "\n",
    "This means, that if the gradient is negative then the update is positive and vice versa. Alpha, the learning rate, makes sure that we actually converge.\n",
    "\n",
    "The needed partial derivatives are calculated usind the chainrule (Backpropagation).\n",
    "\n",
    "This means that for example:\n",
    "\n",
    "$$\\frac{dL}{dw_i} = \\frac{dz(w,b)}{dw_i}\\cdot \\frac{da(z)}{dz} \\cdot \\frac{dL(a, y)}{da}$$ so\n",
    "\n",
    "$$\\frac{dL}{dw_i} = (x_i) \\cdot (a(1-a)) \\cdot (-\\frac{y}{a} + \\frac{1-y}{1-a}) = x_i(a-y)$$\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "As can be seen in the following example, for-loops are around 300 times slower than vectorized calculations, because these can be done in parallel using SIMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For loop: 326.5037536621094ms\n",
      "Vectorized version: 0.9288787841796875ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    a[i]*b[i]\n",
    "toc = time.time()\n",
    "print(\"For loop: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "\n",
    "tic = time.time()\n",
    "np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we use vectors:    \n",
    "\n",
    "$X = (n_x, m)$-Matrix containing the data-samples, one sample each column   \n",
    "$Z = [z_1, z_2, ..., z_m] = w^TX + b$   \n",
    "$A = [a_1, a_2, ..., a_m] = [\\sigma(z_1), \\sigma(z_2), ...] = \\sigma(Z)$   \n",
    "\n",
    "and our code simplifies to\n",
    "\n",
    "\n",
    "$dZ = A - Y$   \n",
    "$dw = \\frac{1}{m}XdZ^T$    \n",
    "$db = \\frac{1}{m}np.sum(dZ)$    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
